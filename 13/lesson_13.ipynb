{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson13:分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是分类\n",
    "\n",
    " - 分类模型：输入样本的属性值，输出对应的类别，将每个样本映射到预先定义好的类别\n",
    " - 属于有监督学习\n",
    "\n",
    "分类算法的实现：\n",
    "\n",
    " - 一：创建分类模型，构建属性与类别的映射关系\n",
    " - 二：使用并优化分类模型\n",
    "\n",
    "常用分类算法\n",
    "\n",
    " - Knn算法\n",
    " - 决策树\n",
    " - 贝叶斯分类器\n",
    " - 支持向量机\n",
    " - 神经网络\n",
    " \n",
    " PS.逻辑回归虽然属于回归类算法，但本质工作也是分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knn算法\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "1. 选取k个和待分类点距离最近的样本点\n",
    "2. 看1中的样本点的分类情况，**投票**决定待分类点所属的类\n",
    "3. 无明确方法决定k取何值最好，只能凭效果来不断优化\n",
    "4. 实现方法： 遍历某一个待测点与其他所有点的距离，然后取距离最近的前K个值，再根据K值查看分类情况。\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41336101-c244ad5c-6f1d-11e8-80ef-ae7d38087c94.png)\n",
    "\n",
    "### 距离概念\n",
    "\n",
    "欧式距离（Euclidean distance）： 直线距离。\n",
    "马氏距离(Mahalanobis distance)： 基于点跟点之间相似度来衡量距离， 用来度量一个样本点P与数据分布为D的集合的距离，也就是数据的协方差距离。\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41336347-97a08156-6f1e-11e8-80c7-3fea0cd9d65f.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 贝叶斯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类器\n",
    "\n",
    "使用条件概率，计算某一个点落在分布$x1还是分布x2的概率更高$\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41336536-1e40176c-6f1f-11e8-961f-828ce47e2fa3.png)\n",
    "\n",
    "\n",
    "## 条件概率公式（贝叶斯公式）\n",
    "\n",
    "设  $D_1,D_2,D_3...D_n  为样本空间S$的一个划分，如果以 $P(D_i) $表示事件 $D_i$ 发生的概率，且   $P(D_i) > 0 (i = 1,2,3,...,n$ 对于任一事件 $x$ ， $P(x) $，则有：\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41336989-7dfe41c8-6f20-11e8-813a-bc41a691d137.png)\n",
    "\n",
    "拓展开来：\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41337178-1696a362-6f21-11e8-84f0-dda8c7a79650.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯网络 BBN\n",
    "\n",
    " - 朴素贝叶斯分类器要求特征之间互相独立，限制了模型的适用性\n",
    " - 用有向无环图表达变量之间的依赖关系，变量用节点表示，依赖关系用边来表示\n",
    " - 顶点、父节点与子节点。在BNN中，如果一个子节点的父节点已知，则该节点的条件独立于它的所有非后代节点。\n",
    " - 每个节点附带一个条件概率表(CPT)，表示该节点与父节点的联系概率\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41337631-57e45d68-6f22-11e8-8ba5-faed434d44c2.png)\n",
    "![image](https://user-images.githubusercontent.com/26344632/41337797-c1661ec0-6f22-11e8-946d-759dc180b9b6.png)\n",
    "![image](https://user-images.githubusercontent.com/26344632/41337842-ec5f3274-6f22-11e8-8993-2cec3defae35.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41337810-ce6787e4-6f22-11e8-8e54-34ef6684f61e.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 决策树\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41338155-e4d2fb52-6f23-11e8-89da-baee139ee9b9.png)\n",
    "\n",
    " - 树中每一个非叶节点表示一个决策，该决策的值导致不同的决策结果(叶节点)或者影响后面的决策选择。\n",
    " - 根据给定的未知分类的元组X，根据其属性值跟踪一条由根节点到叶节点的路径，该叶节点就是该元组的分类结果预测。\n",
    " - $核心在于：空间划分$\n",
    " - 因变量：可以输入一个连续值(引入回归分析），也可以输入一个离散值。\n",
    " \n",
    " ![image](https://user-images.githubusercontent.com/26344632/41338244-2ca466f0-6f24-11e8-859a-1acee93a0e9f.png)\n",
    "\n",
    "\n",
    "常见的决策树算法有:\n",
    "\n",
    " - ID3（-> C4.5  ->C5.0)\n",
    " - CART(Classification And Regression Tree)\n",
    "\n",
    "这两类算法的主要区别在于分裂属性时的选择方法。\n",
    "在构建决策树时，这两类算法的流程基本一样，都采用**贪心方法，自顶而下递归构建决策树**。\n",
    "\n",
    "## 贪心算法\n",
    "\n",
    "---\n",
    "\n",
    "1. 创建一个结点N。如果D中的元组都在同一个类别C中，则N作为叶结点（叶子结点），以C标记；如果属性列表为空，则N作为叶节点，以D中最多的类别C作为标记。\n",
    "2. 根据**分裂准则**找出“最好”的分裂属性A，并用该分裂属性标记N。\n",
    "\n",
    "    1)A是离散的，则A的每个已知值都产生一个分支；\n",
    "    \n",
    "    2)A是连续的，则产生A≤s和A>s两个分支；\n",
    "    \n",
    "    3)若A是连续的，并且必须产生二叉树，则产生A∈A1和A∈A2两个分支，其中A1，A2非空且A1∪A2=A\n",
    "    \n",
    "3. 若给定的分支中的元组非空，对于D的每一个分支Dj，重复步骤1,2\n",
    "\n",
    "PS. \n",
    "\n",
    "    D：数据分区 Datasegment\n",
    "\n",
    "    C：类别 Classification\n",
    "\n",
    "    A：属性 Attribute\n",
    "\n",
    "    N：结点 Node 区别 节点\n",
    "\n",
    "    叶子结点（叶子结点 leaf node）：终端节点，无需再进行分类的节点\n",
    "\n",
    "    叶子节点：两线相交时中间的点\n",
    "\n",
    "### 属性选择度量（分裂准则）\n",
    "\n",
    " - 根据分裂准则把D划分为较小的分区，最好的情况是每个分区都是纯的，即落在一个给定分区的所有元组都是相同的类。\n",
    "     - **最好的分裂准则就是令到每个分区尽量的纯。**\n",
    " - 属性选择度量给学习集中的每个属性提供了评定。\n",
    "     - **有最好度量得分的属性被选为分裂属性。**\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### *ID3系列算法：基于熵\n",
    "\n",
    "1948年，香农提出了“信息熵”的概念，解决了对系统信息的量化度量问题。\n",
    "\n",
    "使用信息增益（最大）来判断最佳分裂属性的标准\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41341489-657b47d4-6f2c-11e8-9c00-d0cdce1c5475.png)\n",
    "\n",
    "#### *信息熵\n",
    "\n",
    "对D中的元组分类所需要的期望信息（信息熵）：\n",
    "![image](https://user-images.githubusercontent.com/26344632/41341606-b3678372-6f2c-11e8-9e01-e74ce71aef04.png)\n",
    "\n",
    "其中， 是$D中任意元组属于类别C_i的概率，使用|Ci,D|/|D|$来估计。\n",
    "\n",
    "若现在按照某属性A划分D中的元组。若A是离散的，根据其取值将$D分为v$个子集。在\n",
    "\n",
    "此划分后对元组分类所需要的期望信息为：\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41341620-ba6b252a-6f2c-11e8-8992-f187629af3d7.png)\n",
    "\n",
    "其中，项$|Dj|/|D|$表示第j个分区的权重。\n",
    "\n",
    "#### *信息增益\n",
    "\n",
    "信息增益定义为原来的信息需求与新的信息需求之间的差，即\n",
    "\n",
    " $Gain(A)=Info(D)-InfoA(D)$\n",
    "\n",
    "$Gain(A)$表示知道A的值而导致的信息需求的期望减少。\n",
    "\n",
    "选择具有最大信息增益的属性作为分裂属性.\n",
    "\n",
    "对于连续值属性A，要计算它的信息增益，其实也等价于寻找A的“最佳”分裂点。\n",
    "\n",
    "1. 将A的值按递增排序\n",
    "2. 每对相邻值的中点（v个）是一个可能的分裂点。将A按照这些分裂点做v-1次划分，计算每次划分的InfoA(D)\n",
    "3. 选择具有最小期望信息需求的点作为A的分裂点，并根据该分裂点计算A的信息增益\n",
    "\n",
    "### 缺陷\n",
    "\n",
    "信息增益存在着一定的局限性，它会倾向于选择具有大量值的属性，但是有时候这种属性对分类其实没有多大作用。\n",
    "例如每个学生的学号对其成绩高低的分类没有作用，但是如果利用信息增益来选择分裂属性，学号这一属性的划分会导致大量分区，每一个分区都只有一个学生所以每个分区都是纯的，但是这样的划分对分辨学生成绩的高低并没有用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化方法\n",
    "\n",
    "1. C4.5——增益率\n",
    "\n",
    "    为了克服这一不足，C4.5算法中使用了分裂信息值将信息增益规范化。\n",
    "\n",
    "    ![image](https://user-images.githubusercontent.com/26344632/41341868-5d788a5a-6f2d-11e8-8452-aeae8a15d5ff.png)\n",
    "\n",
    "    信息增益率定义为\n",
    "\n",
    "    ![image](https://user-images.githubusercontent.com/26344632/41341882-67412a60-6f2d-11e8-8f70-f8b71cf68c27.png)\n",
    "\n",
    "\n",
    "选择具有最大增益率的属性作为分裂属性。\n",
    "    \n",
    "2. CART(Classification And Regression Tree)算法\n",
    "\n",
    "    与ID3算法的差异：\n",
    "    1. 不是基于信息熵，而是基于不纯度来评定属性\n",
    "    2. 严格的二元划分。使用ID3算法有可能会产生多叉树，但是使用CART算法只产生二叉树\n",
    "    3. 根据y值类型的不同可分为**回归树和分类树**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 分类树和回归树\n",
    "\n",
    "---\n",
    "\n",
    " - 分类树：y值是类别\n",
    " - 回归树：y值是实数\n",
    " - 异同：\n",
    "    1. 所用算法思路一致\n",
    "    2. 评定分裂标准不一样\n",
    " \n",
    "## 分类树（离散）\n",
    "\n",
    "---\n",
    "\n",
    "### 基尼系数\n",
    "使用基尼指数（Gini Index）来衡量数据的不纯度\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41342093-f7199816-6f2d-11e8-85ab-199623ca3084.png)\n",
    "\n",
    "其中，$p_i是D中元组属于类别Ci的概率，使用|Ci,D|/|D|来估计$\n",
    "\n",
    "$Gini(D)越小，表示D越纯$\n",
    "\n",
    "### 基尼指数\n",
    "\n",
    "对于某个属性A的基尼指数的计算。\n",
    "\n",
    "因为CART算法是生成二叉树，所以如果A的二元划分将D划分为D1和D2，则给定该划分，基尼指数是：\n",
    "![image](https://user-images.githubusercontent.com/26344632/41342146-1cf003cc-6f2e-11e8-94b1-6077bf4cbd9c.png)\n",
    "\n",
    " - 对于离散属性：选择产生最小基尼指数的子集作为该属性的分裂子集\n",
    "\n",
    " - 对于连续属性：考虑每个可能的分裂点，选择产生最小基尼指数的点作为该属性的分裂点\n",
    "\n",
    " - 对于属性A的二元划分导致的不纯度降低为\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41343106-4f678f8a-6f30-11e8-8d74-75952762478c.png)\n",
    "\n",
    "选择具有最大的不纯度降低的属性和分裂点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归树（连续）\n",
    "\n",
    "---\n",
    "\n",
    "**回归树并不能做回归分析，只能给某一个变量的均值范围大致在哪里的判断**\n",
    "\n",
    "将空间中的点划分成不同的区域，同一个区域中的点拥有相同的水平(均值）\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41343306-cb4b2de6-6f30-11e8-8b5a-1b2e1d629e96.png)\n",
    "\n",
    "### RSS 残差平方和\n",
    "\n",
    "将空间分成J个区域，第j个区域记为Rj\n",
    "使用统计量RSS去评价回归树。RSS越小，回归树越好。所以目标就是寻求一个最佳的\n",
    "分法，使得RSS最小。\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41343334-dec00cfc-6f30-11e8-9136-314f09f8e5ba.png)\n",
    "\n",
    "其中，![image](https://user-images.githubusercontent.com/26344632/41343413-1dd2f904-6f31-11e8-8684-120bec54ad2f.png)表示第j个区域中y的均值\n",
    "\n",
    "### 寻找最佳分裂点：\n",
    "1.对于分裂点s，属性Aj的二元划将D分为D1={Ai≤s}和D2={Ai>s}\n",
    "2.此时RSS的计算公式等价于\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41343433-2cfca178-6f31-11e8-96a1-6ec8c969fb67.png)\n",
    "\n",
    "要寻求适当的s和i使得上述公式最小\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41343697-dadda530-6f31-11e8-9a04-3b3fe9486895.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 剪枝\n",
    "---\n",
    "\n",
    "避免过度拟合，简化模型\n",
    "\n",
    "两种修剪方法：先剪枝与后剪枝\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41343854-43232f02-6f32-11e8-9c97-e93b74fb7937.png)\n",
    "\n",
    "先剪枝：\n",
    "\n",
    "    通过设定一定的阀值来停止树的生长。\n",
    "    例如，在构建树模型时，使用信息增益、基尼指数来度量划分的优劣。可以预先设定一个阀值，当划分一个结点的元组到时低于预设的阀值时，停止该子集的划分。\n",
    "\n",
    "后剪枝：\n",
    "\n",
    "    1. 等树完全生成后再通过删除结点去修剪决策树。\n",
    "    2. 由于先剪枝中，选择合适的阀值存在一定的困难，所以后剪枝更加常用\n",
    "    3. 实现代表：\n",
    "\n",
    "        CART——代价复杂度法\n",
    "\n",
    "        使用参数α去衡量代价与复杂度之间关系\n",
    "\n",
    "   ![image](https://user-images.githubusercontent.com/26344632/41343974-8cae80cc-6f32-11e8-92c1-6dc24244796c.png)\n",
    "\n",
    "\n",
    "        其中，|T|是树的结点数，用于衡量树的复杂度。\n",
    "\n",
    "        通过给定α的值去计算树的最佳结点数，将多余的结点删除得到修剪后的树\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树的优点：\n",
    "\n",
    "1. 树模型十分通俗易懂，解释起来简单明了\n",
    "2. 相对于其他模型，树模型可以通过图形模型展示，即使不具备相应专业知识的人可以可以一名了然\n",
    "3. 树模型可以直接处理定性变量，不需要增加虚拟变量\n",
    "\n",
    "#### 决策树的缺点：\n",
    "\n",
    "准确率不够高，可通过bagging，xgboost，随机森林来提升准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 支持向量机 SVM ——从线性判别法说起\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性判别法（简易版本的SVM）\n",
    "\n",
    "用一条直线来划分学习集（这条直线不一定存在），然后根据待测点在直线的哪一边决定它的分类\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41406774-f448a8f2-6fff-11e8-86c8-5a72a67ff756.png)\n",
    "![image](https://user-images.githubusercontent.com/26344632/41406780-fa1e9e6c-6fff-11e8-948e-fb2c85d7396b.png)\n",
    "![a](https://user-images.githubusercontent.com/26344632/41407032-c55f7146-7000-11e8-94bf-ae6725b8570e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机 SVM\n",
    "\n",
    " - 原创性（非组合）的具有明显直观几何意义的分类算法，具有较高的准确率\n",
    " - 思想直观，但细节异常复杂，内容涉及凸分析算法，核函数，神经网络等高深的领域，几乎可以写成单独的大部头与著。大部分非与业人士会觉得难以理解。\n",
    "\n",
    "两种情况\n",
    "\n",
    " - 简单情况：线性可分，把问题转化为一个凸优化问题，可以用拉格朗日乘子法简化，然后用既有的算法解决\n",
    " - 复杂情况：线性不可分，用映射函数将样本投射到高维空间，使其变成线性可分的情形。利用核函数来减少高维度计算量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优分割平面\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41407162-2f866fd4-7001-11e8-9e2e-4425f8b28777.png)\n",
    "\n",
    "最大边缘超平面(MMH)\n",
    "\n",
    " - 寻找一条直线，使得直线距离两边的点的距离相同且最大化\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41407285-8a83f884-7001-11e8-89bb-21211ab43c1d.png)\n",
    "\n",
    "\n",
    "线性不可分曲线\n",
    " - 把二维中线性不可分的点，投射到三维空间中，再进行线性划分\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41407376-ca8ca7c8-7001-11e8-9288-2ffe43b99110.png)![image](https://user-images.githubusercontent.com/26344632/41407449-07f997b0-7002-11e8-93b4-0e89a39f9439.png)![image](https://user-images.githubusercontent.com/26344632/41407463-12b66250-7002-11e8-9868-f6afdc91c642.png)![image](https://user-images.githubusercontent.com/26344632/41407477-1e466bd8-7002-11e8-9b95-539202dfc3e4.png)![image](https://user-images.githubusercontent.com/26344632/41407486-263d034c-7002-11e8-8c66-238e529ed759.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
