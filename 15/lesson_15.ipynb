{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lesson_15 降维技术\n",
    "\n",
    "\n",
    "\n",
    "为何要降维？\n",
    "\n",
    "数据如果变量多、维度高，计算难度大，对参数估计的不确定增加，容易变成系数矩阵，难以计算出显著的计算结果。\n",
    "\n",
    "降维的目的有二\n",
    " - 对数据进行可视化，以便对数据进行观察和探索\n",
    " - 简化机器学习模型的训练和预测\n",
    "\n",
    "我们很难对高维数据具有直观的认识，如果把数据的维度降低到2维或者3维，并且保持数据点的关系，与原高维空间里的关系，保持不变或者近似，我们就可以进行可视化，肉眼来观察数据。\n",
    "\n",
    "数据经过降维以后，如果保留了原有数据的主要信息，那么我们就可以用降维的数据进行机器学习模型的训练和预测，由于数据量大大缩减，训练和预测的时间效率将大为提高。\n",
    "     \n",
    "\n",
    "做法:把数据从高维空间映射到一个低维空间，尽可能保留原有信息。\n",
    "\n",
    " - 使得数据集更易使用\n",
    " - 降低算法计算开销\n",
    " - 去除噪声\n",
    " - 使得结果易懂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "主成分分析是一种多变量统计方法,首先由K.皮尔森（Karl Pearson）对非随机变量引入的，尔后H.霍特林将此方法推广到随机向量的情形。\n",
    "\n",
    "主成分分析法（Principal Component Analysis，PCA）是一种降维的统计方法，它借助于一个**正交变换（坐标系转换)**，将其分量相关的原随机向量转化成其分量不相关的新随机向量。\n",
    "\n",
    "**这在代数上表现为将原随机向量的协方差阵变换成对角形阵**，**在几何上表现为将原坐标系变换成新的正交坐标系，使之指向样本点散布最开的p 个正交方向**，然后对多维变量系统进行降维处理，使之能以一个较高的精度转换成低维变量系统，再通过构造适当的价值函数，进一步把低维系统转化成一维系统。\n",
    "\n",
    "在实际课题中，为了全面分析问题，往往提出很多与此有关的变量（或因素），因为每个变量都在不同程度上反映这个课题的某些信息。信息的大小通常用离差平方和或方差来衡量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "\n",
    "在用统计分析方法研究多变量的课题时，变量个数太多就会增加课题的复杂性。人们自然希望变量个数较少而得到的信息较多。\n",
    "\n",
    "在很多情形，变量之间是有一定的相关关系的，当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。\n",
    "\n",
    "**主成分分析是对于原先提出的所有变量，将重复的变量（关系紧密的变量）删去多余，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。**\n",
    "\n",
    "设法将原来变量重新组合成一组新的互相无关的几个综合变量，同时根据实际需要从中可以取出几个较少的综合变量尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，也是数学上用来降维的一种方法。\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41953385-f9f4c47c-7a07-11e8-9d69-761ec113cb90.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本思想\n",
    "\n",
    "**主成分分析是设法将原来众多具有一定相关性（比如P个指标）的多个指标，重新组合成一组新的互相无关的综合指标来代替原来的指标。**\n",
    "\n",
    "主成分分析，是考察多个变量间相关性一种多元统计方法，研究如何通过少数几个主成分来揭示多个变量间的内部结构，即从原始变量中导出少数几个主成分，使它们尽可能多地保留原始变量的信息，且彼此间相互独立(互不相关)。\n",
    "\n",
    "通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。\n",
    "\n",
    "### 步骤\n",
    "\n",
    "进行主成分分析主要步骤如下：\n",
    "\n",
    "1. 指标数据标准化（SPSS软件自动执行）；\n",
    "2. 指标之间的相关性判定；\n",
    "3. 确定主成分个数m；\n",
    "4. 主成分Fi表达式；\n",
    "5. 主成分Fi命名。 \n",
    "\n",
    "### 数学上详解\n",
    "\n",
    "最经典的做法就是用F1（选取的第一个线性组合，即第一个综合指标）的**方差**来表达，即$Var(F1)$越大，表示F1包含的信息越多。因此在所有的线性组合中选取的F1应该是方差最大的，故称F1为第一主成分。\n",
    "\n",
    "如果第一主成分不足以代表原来P个指标的信息，再考虑选取F2即选第二个线性组合，为了有效地反映原来信息，F1已有的信息就不需要再出现在F2中，用数学语言表达就是要求$Cov(F1, F2)=0$，则称F2为第二主成分，依此类推可以构造出第三、第四，……，第P个主成分。\n",
    "\n",
    "$F_p = a_{1i}*Z_{X1} + a_{2i}*Z_{X2} + …… + a_{pi}*Z_{Xp}$\n",
    "\n",
    "$其中 a_{1i}, a_{2i}, ……, a_{pi}(i=1,……,m)为X$的**协方差阵Σ的特征值所对应的特征向量**。\n",
    "\n",
    "$Z_{X1}, Z_{X2}, ……, Z_{Xp}是原始变量经过标准化处理的值，因为在实际应用中，往往存在指标的量纲不同，所以在计算之前须先消除量纲的影响，而将原始数据标准化。$\n",
    "\n",
    "$本文所采用的数据就存在量纲影响[注：本文指的数据标准化是指Z标准化]。$\n",
    "\n",
    "$A = (a_{ij})p×m = (a_1,a_2,…a_m)，R_{ai} = λ_ia_i，R为相关系数矩阵$，$λi、ai$是相应的** 特征值和单位特征向量**，$λ1 ≥ λ2 ≥ …≥ λp ≥ 0 。$\n",
    "\n",
    "### 要点\n",
    "\n",
    "1. 通过析取主成分显出最大的个别差异，也用来削减回归分析和聚类分析中变量的数目。\n",
    "2. 可以使用样本协方差矩阵或相关系数矩阵作为出发点进行分析。\n",
    "3. 成分的保留：Kaiser主张（1960）将特征值小于1的成分放弃，只保留特征值大于1的成分。如果能用不超过3-5个成分就能解释方差变异的80%，就算是成功。\n",
    "4. 通过对原始变量进行线性组合，得到优化的指标：把原先多个指标的计算降维为少量几个经过优化指标的计算（占去绝大部分份额）\n",
    "5. 设法将原先众多具有一定相关性的指标，重新组合为一组新的互相独立的综\n",
    "合指标，并代替原先的指标\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41953542-e7088af0-7a08-11e8-9e44-4c40834b77f9.png)\n",
    "\n",
    "详解: https://www.cnblogs.com/Ponys/p/3428270.html\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954406-b4585df6-7a0d-11e8-8a1c-a0d29d8dbbe7.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954293-2b40ab04-7a0d-11e8-9d29-c745013b014c.png)\n",
    "\n",
    "### 旋转前\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954343-723729ac-7a0d-11e8-9ace-09fbbaeba71d.png)\n",
    "\n",
    "### 旋转后\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954386-a68dec90-7a0d-11e8-988c-fe660e5ae760.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954354-7f9b472c-7a0d-11e8-9ce3-0b2af707f712.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954425-c9c90f46-7a0d-11e8-803c-a2712f2ae132.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954437-d978a898-7a0d-11e8-92a2-4b8739cfbe7e.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954443-e5b38c36-7a0d-11e8-8bd4-08a3ad914029.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 主成分分析的主要作用\n",
    "\n",
    "概括起来说，主成分分析主要由以下几个方面的作用。\n",
    "\n",
    "1. 主成分分析能降低所研究的数据空间的维数。即用研究m维的Y空间代替p维的X空间(m<p)，而低维的Y空间代替高维的x空间所损失的信息很少。即：使只有一个主成分Yl(即 m=1)时，这个Yl仍是使用全部X变量(p个)得到的。例如要计算Yl的均值也得使用全部x的均值。在所选的前m个主成分中，如果某个Xi的系数全部近似于零的话，就可以把这个Xi删除，这也是一种删除多余变量的方法。\n",
    "\n",
    "2. 有时可通过因子负荷aij的结论，弄清X变量间的某些关系。\n",
    "\n",
    "3. 多维数据的一种图形表示方法。我们知道当维数大于3时便不能画出几何图形，多元统计研究的问题大都多于3个变量。要把研究的问题用图形表示出来是不可能的。然而，经过主成分分析后，我们可以选取前两个主成分或其中某两个主成分，根据主成分的得分，画出n个样品在二维平面上的分布况，由图形可直观地看出各样品在主分量中的地位，进而还可以对样本进行分类处理，可以由图形发现远离大多数样本点的离群点。\n",
    "\n",
    "4. 由主成分分析法构造回归模型。即把各主成分作为新自变量代替原来自变量x做回归分析。\n",
    "\n",
    "5. 用主成分分析筛选回归变量。回归变量的选择有着重的实际意义，为了使模型本身易于做结构分析、控制和预报，好从原始变量所构成的子集合中选择最佳变量，构成最佳变量集合。用主成分分析筛选变量，可以用较少的计算量来选择量，获得选择最佳变量子集合的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优缺点\n",
    "\n",
    "优点：降低数据的复杂性，识别最重要的多个特征\n",
    "\n",
    "缺点：不一定需要，且有可能损失有用信息\n",
    "\n",
    "适用数据类型：狭长型的数据（数值型数据），球型数据则不适合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算过程\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41953740-dc46f4f2-7a09-11e8-8fbd-90a6add49e7b.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python中的PCA实现\n",
    "```\n",
    "import sklearn as sk\n",
    "\n",
    "sk.PCA(n_components=None, # 需要保留的主成分数目; \"mle\" 自动选择最佳的主成分数据; 0~1:例如0.8,表示选择解释超过80%的主成分数量\n",
    "       copy=True, # 不覆盖原数据\n",
    "       whiten=False) # FALSE不白化，保留每个特征的方差（保留精度）;True白化，使得每个特征具有相同的方差。\n",
    "```\n",
    "\n",
    "属性\n",
    "\n",
    " - components\n",
    " - explained_variance_ratio\n",
    " - mean_\n",
    " - n_components\n",
    " - noise_variance\n",
    "\n",
    "方法\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954010-78104fae-7a0b-11e8-88b2-27aa50b4fa26.png)\n",
    "\n",
    "参考链接:\n",
    "\n",
    "参数详解:http://www.cnblogs.com/eczhou/p/5433856.html\n",
    "\n",
    "白化： http://deeplearning.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python实例：协方差矩阵、特征值与特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#协方差矩阵\n",
    "import numpy as np\n",
    "X = [[2, 0, -1.4],\n",
    "[2.2, 0.2, -1.5],\n",
    "[2.4, 0.1, -1],\n",
    "[1.9, 0, -1.2]]\n",
    "print(np.cov(np.array(X).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征值与特征向量\n",
    "w, v = np.linalg.eig(np.array([[1, -2], [2, -3]]))\n",
    "print('特征值：{}\\n特征向量：{}'.format(w,v))\n",
    "\n",
    "#\n",
    "a = [[-0.27, -0.3],\n",
    "[1.23, 1.3],\n",
    "[0.03, 0.4],\n",
    "[-0.67, 0.6],\n",
    "[-0.87, 0.6],\n",
    "[0.63, 0.1],\n",
    "[-0.67, -0.7],\n",
    "[-0.87, -0.7],\n",
    "[1.33, 1.3],\n",
    "[0.13, -0.2]]\n",
    "b = [[0.73251454], [0.68075138]]\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA实例：鸢尾花数据集降维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#鸢尾花数据集的降维\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "y = data.target\n",
    "X = data.data\n",
    "pca = PCA(n_components=2)\n",
    "reduced_X = pca.fit_transform(X)\n",
    "\n",
    "red_x, red_y = [], []\n",
    "blue_x, blue_y = [], []\n",
    "green_x, green_y = [], []\n",
    "for i in range(len(reduced_X)):\n",
    "    if y[i] == 0:\n",
    "        red_x.append(reduced_X[i][0])\n",
    "        red_y.append(reduced_X[i][1])\n",
    "    elif y[i] == 1:\n",
    "        blue_x.append(reduced_X[i][0])\n",
    "        blue_y.append(reduced_X[i][1])\n",
    "    else:\n",
    "        green_x.append(reduced_X[i][0])\n",
    "        green_y.append(reduced_X[i][1])\n",
    "plt.scatter(red_x, red_y, c='r', marker='x')\n",
    "plt.scatter(blue_x, blue_y, c='b', marker='D')\n",
    "plt.scatter(green_x, green_y, c='g', marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因子分析\n",
    "\n",
    "### 简介\n",
    "\n",
    " - 降维的一种方法，是主成分分析的推广和发展\n",
    " - 是用于分析隐藏在表面现象背后的因子作用的统计模型。试图用最少个数的不可测的公共因子的线性函数与特殊因子之和来描述原来观测的每一分量\n",
    "     - 例子：各科学习成绩（数学能力，语言能力，运动能力等）\n",
    "     - 例子：生活满意度（工作满意度，家庭满意度）\n",
    "\n",
    "#### 来源\n",
    "\n",
    "因子分析是指研究从**变量群中提取共性因子**的统计技术。最早由英国心理学家C.E.斯皮尔曼提出。他发现学生的各科成绩之间存在着一定的相关性，一科成绩好的学生，往往其他各科成绩也比较好，从而推想是否存在某些潜在的共性因子，或称某些一般智力条件影响着学生的学习成绩。因子分析可在许多变量中找出**隐藏的具有代表性的因子**。\n",
    "将相同本质的变量归入一个因子，可减少变量的数目，还可检验变量间关系的假设。\n",
    "\n",
    "#### 主要用途\n",
    "\n",
    " - 减少分析变量个数\n",
    " - 通过对变量间相关关系的探测，将原始变量分组，即将相关性高的变量分为一组，用共性因子来代替该变量\n",
    " - 使问题背后的业务因素的意义更加清晰呈现\n",
    " \n",
    "#### 与PCA的区别\n",
    "\n",
    "主成分分析侧重“变异量”，通过转换原始变量为新的组合变量使到数据的“变异量”最大，从而能把样本个体之间的差异最大化，但得出来的主成分往往从业务场景的角度难以解释\n",
    "\n",
    "因子分析更重视相关变量的“共变异量”，组合的是相关性较强的原始变量，目的是找到在背后起作用的少量关键因子，因子分析的结果往往更容易用业务知识去加以解释\n",
    "\n",
    "因子分析采用了更复杂的数学模型\n",
    "\n",
    " - 比主成分分析更加复杂的数学模型\n",
    " - 求解模型的方法：主成分法，主因子法，极大似然法\n",
    " - 结果还可以通过因子旋转，使到业务意义更加明显\n",
    "\n",
    "\n",
    "#### 分析描述\n",
    "\n",
    "验证性因子分析(confirmatory factor analysis) 的强项正是在于它允许研究者明确描述一个理论模型中的细节。那么一个研究者想描述什么呢？\n",
    "\n",
    "我们曾经提到因为测量误差的存在，研究者需要使用多个测度项。当使用多个测度项之后，我们就有测度项的“质量”问题，即有效性检验。\n",
    "\n",
    "而有效性检验就是要看一个测度项是否与其所设计的因子有显著的载荷，并与其不相干的因子没有显著的载荷。\n",
    "\n",
    "当然，我们可能进一步检验一个测度项工具中是否存在单一方法偏差，一些测度项之间是否存在“子因子”。\n",
    "\n",
    "这些测试都要求研究者明确描述测度项、因子、残差之间的关系。对这种关系的描述又叫测度模型 (measurement model)。\n",
    "\n",
    "对测度模型的质量检验是假设检验之前的必要步骤。\n",
    "\n",
    "验证性因子分析往往用**极大似然估计法**求解。它往往与**结构方程**的方法连用。具体的使用过程与原理可以参看扩展阅读中的《社会调查研究方法》。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学模型\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/42022669-73a222ae-7af0-11e8-8499-a623f8b66d64.png)\n",
    "\n",
    "因子分析是 把原变量 变成 该变量的均值+共同因子+隐藏因子的线性组合\n",
    "\n",
    "$μ$：均值\n",
    "\n",
    "$A$：因子载荷矩阵\n",
    "\n",
    "$F$：共同因子，$f_1,f_2,...,f_m$是共同因子向量，m<p；一般来说每个公共因子至少对两个原始变量起作用，如果只对一个原始变量起作用，则归为特殊因子。\n",
    "\n",
    "\n",
    "$ε$：特殊因子，$ε_1,ε_2,...,ε_p$是特殊因子向量，是不可观测的随机变量。\n",
    "\n",
    "\n",
    "$X = μ + AF + ε$:以矩阵的形式表达因子分析,原始因子X = 均值μ+共同因子AF+特殊因子ε\n",
    "\n",
    "\n",
    "$E(F)=0$：假设共同因子F的均值为0\n",
    "\n",
    "$E(ﻉ)$：假设特殊因子ﻉ的均值为0\n",
    "\n",
    "$Var(F) = I_m$ ：假设共同因子F的方差是一个单位向量，所有共同因子的方差加起来=1，不同共同因子之间互不相关，协方差=0。\n",
    "\n",
    "$Var(ε)=D = diag(δ_1^2,δ_2^2,δ_p^2) $：假设不同特殊因子之间互不相关，不同特殊因子之间互不相等。\n",
    "\n",
    "$Cov(F,ε) = 0$：共同因子与特殊因子之间协方差为0，即互不相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/26344632/42022691-833cc8cc-7af0-11e8-83dc-de7780d90540.png)\n",
    "\n",
    "原始变量矩阵Σ = AA^T*+D（因子载荷矩阵 * 转置的因子载荷矩阵 +常数矩阵）\n",
    "\n",
    "不受到单位换算的影响，因为因子与均值都会同时乘以单位换算量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数学解析\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/42022286-807e8b44-7aef-11e8-9f9d-e7f4d30507a1.png)\n",
    "\n",
    "### 统计意义\n",
    "\n",
    "因子载荷:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/42028421-d4e2a51c-7afe-11e8-9043-35ba7f3723cb.png)\n",
    "\n",
    "共同度：$h_i^2 反映了公共因子f_i对x_i的贡献度$。\n",
    "\n",
    "特殊方差：![image](https://user-images.githubusercontent.com/26344632/42028493-ffaa761c-7afe-11e8-9868-52e24858fad7.png)\n",
    "\n",
    "总方差贡献:![image](https://user-images.githubusercontent.com/26344632/42028571-342264b8-7aff-11e8-8aae-b3e40aff82d2.png)\n",
    "\n",
    "因子分析的任务就是从X的相关矩阵和![image](https://user-images.githubusercontent.com/26344632/42022540-251cf53c-7af0-11e8-980c-dfa620499140.png) 出发，通过方差最大的正交旋转，求出矩阵A的各列，使相应的“贡献”有顺序 ![image](https://user-images.githubusercontent.com/26344632/42022553-2a6fb664-7af0-11e8-9891-a36a94e1ac6c.png)\n",
    "\n",
    "## 因子载荷矩阵A 和 特殊方差矩阵D的估计\n",
    "\n",
    "主成分法：\n",
    "\n",
    " - 通过样本估算期望和协方差阵\n",
    " - 求协方差阵的特征值和特征向量\n",
    " - 省去特征值较小的部分，求出A、D\n",
    "\n",
    "主因子法：\n",
    "\n",
    " - 首先对变量标准化\n",
    " - 给出m个特殊方差的估计（初始）值\n",
    " - 求出简约相关阵R*（p阶方阵）\n",
    " - 计算R*的特征值和特征向量，取其前m个，略去其它部分\n",
    " - 求出A*和D*，再迭代计算\n",
    "\n",
    "极大似然法\n",
    "\n",
    " - 似然函数\n",
    " - 极大似然函数\n",
    " - 算法描述（薛毅书p533）\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/42028805-f9df9040-7aff-11e8-9388-23c315eea574.png)\n",
    "\n",
    "## 对因子载荷矩阵进行方差最大的正交旋转\n",
    "\n",
    " - 由于因子载荷矩阵不是唯一，有时因子的实际意义会变得难以解释。\n",
    " - 因子载荷矩阵的正交旋转\n",
    " - 因子载荷方差\n",
    " - 载荷值趋于1或趋于0，公共因子具有简单化的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 隐性变量\n",
    "\n",
    "因子分析的主要目的是用来描述隐藏在一组测量到的变量中的一些更基本的，但又无法直接测量到的隐性变量 (latent variable, latent factor)。比如，如果要测量学生的学习积极性(motivation)，课堂中的积极参与，作业完成情况，以及课外阅读时间可以用来反应积极性。而学习成绩可以用期中，期末成绩来反应。在这里，学习积极性与学习成绩是无法直接用一个测度(比如一个问题) 测准，它们必须用一组测度方法来测量，然后把测量结果结合起来，才能更准确地把握。换句话说，这些变量无法直接测量。可以直接测量的可能只是它所反映的一个表征(manifest)，或者是它的一部分。在这里，表征与部分是两个不同的概念。表征是由这个隐性变量直接决定的。隐性变量是因，而表征是果，比如学习积极性是课堂参与程度 (表征测度)的一个主要决定因素。\n",
    "因子分析是社会研究的一种有力工具，但不能肯定地说一项研究中含有几个因子，当研究中选择的变量变化时，因子的数量也要变化。此外对每个因子实际含意的解释也不是绝对的。\n",
    "\n",
    "#### 得到因子\n",
    "因子分析的方法有两类。一类是探索性因子分析法，另一类是验证性因子分析。探索性因子分析不事先假定因子与测度项之间的关系，而让数据“自己说话”。主成分分析和共因子分析是其中的典型方法。验证性因子分析假定因子与测度项的关系是部分知道的，即哪个测度项对应于哪个因子，虽然我们尚且不知道具体的系数。\n",
    "\n",
    "#### 验证因子\n",
    "探索的因子分析有一些局限性。第一，它假定所有的因子(旋转后) 都会影响测度项。在实际研究中，我们往往会假定一个因子之间没有因果关系，所以可能不会影响另外一个因子的测度项。第二，探索性因子分析假定测度项残差之间是相互独立的。实际上，测度项的残差之间可以因为单一方法偏差、子因子等因素而相关。第三，探索性因子分析强制所有的因子为独立的。这虽然是求解因子个数时不得不采用的机宜之计，却与大部分的研究模型不符。最明显的是，自变量与应变量之间是应该相关的，而不是独立的。这些局限性就要求有一种更加灵活的建模方法，使研究者不但可以更细致地描述测度项与因子之间的关系，而且可以对这个关系直接进行测试。而在探索性因子分析中，一个被测试的模型(比如正交的因子) 往往不是研究者理论中的确切的模型。\n",
    "\n",
    "\n",
    "#### 因子应用\n",
    "在市场调研中，研究人员关心的是一些研究指标的集成或者组合，这些概念通常是通过等级评分问题来测量的，如利用李克特量表取得的变量。每一个指标的集合（或一组相关联的指标）就是一个因子，指标概念等级得分就是因子得分。\n",
    "因子分析在市场调研中有着广泛的应用，主要包括：\n",
    "\n",
    "（1）消费者习惯和态度研究（U&A）\n",
    "\n",
    "（2） 品牌形象和特性研究\n",
    "\n",
    "（3）服务质量调查\n",
    "\n",
    "（4） 个性测试\n",
    "\n",
    "（5）形象调查\n",
    "\n",
    "（6） 市场划分识别\n",
    "\n",
    "（7）顾客、产品和行为分类\n",
    "\n",
    "在实际应用中，通过因子得分可以得出不同因子的重要性指标，而管理者则可根据这些指标的重要性来决定首先要解决的市场问题或产品问题。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数\n",
    "\n",
    "```\n",
    "sklearn.decomposition.FactorAnalysis(n_components=None, \n",
    "                                     tol=0.01, \n",
    "                                     copy=True, # 保留原x值\n",
    "                                     max_iter=1000,  # 最大迭代次数\n",
    "                                     noise_variance_init=None, # 特殊方差的初始值 \n",
    "                                     svd_method='randomized', # 快速svd分解（奇异值分解），还有lapack(标准svd)这一方法\n",
    "                                     iterated_power=3, #幂方的迭代次数\n",
    "                                     random_state=0 # 设置随机因子的值)\n",
    "                                     \n",
    "```\n",
    "                                     \n",
    "属性\n",
    "\n",
    " - components\n",
    " - loglike\n",
    " - noise_variance\n",
    " - n_iter\n",
    "\n",
    "方法\n",
    "\n",
    " - fit\n",
    " - scorce\n",
    " - fit_transform\n",
    " - scorce_samples\n",
    " - get_covariance\n",
    " - set_params\n",
    " - get_params\n",
    " - transform\n",
    " - get_precision\n",
    " \n",
    " 对svd奇异值的解释:https://blog.csdn.net/xiaocong1990/article/details/54909126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
