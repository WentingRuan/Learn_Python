{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lesson_15 降维技术\n",
    "\n",
    "\n",
    "\n",
    "为何要降维？\n",
    "\n",
    "数据如果变量多、维度高，计算难度大，对参数估计的不确定增加，容易变成系数矩阵，难以计算出显著的计算结果。\n",
    "\n",
    "降维的目的有二\n",
    " - 对数据进行可视化，以便对数据进行观察和探索\n",
    " - 简化机器学习模型的训练和预测\n",
    "\n",
    "我们很难对高维数据具有直观的认识，如果把数据的维度降低到2维或者3维，并且保持数据点的关系，与原高维空间里的关系，保持不变或者近似，我们就可以进行可视化，肉眼来观察数据。\n",
    "\n",
    "数据经过降维以后，如果保留了原有数据的主要信息，那么我们就可以用降维的数据进行机器学习模型的训练和预测，由于数据量大大缩减，训练和预测的时间效率将大为提高。\n",
    "     \n",
    "\n",
    "做法:把数据从高维空间映射到一个低维空间，尽可能保留原有信息。\n",
    "\n",
    " - 使得数据集更易使用\n",
    " - 降低算法计算开销\n",
    " - 去除噪声\n",
    " - 使得结果易懂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "主成分分析是一种多变量统计方法,首先由K.皮尔森（Karl Pearson）对非随机变量引入的，尔后H.霍特林将此方法推广到随机向量的情形。\n",
    "\n",
    "主成分分析法（Principal Component Analysis，PCA）是一种降维的统计方法，它借助于一个**正交变换（坐标系转换)**，将其分量相关的原随机向量转化成其分量不相关的新随机向量。\n",
    "\n",
    "**这在代数上表现为将原随机向量的协方差阵变换成对角形阵**，**在几何上表现为将原坐标系变换成新的正交坐标系，使之指向样本点散布最开的p 个正交方向**，然后对多维变量系统进行降维处理，使之能以一个较高的精度转换成低维变量系统，再通过构造适当的价值函数，进一步把低维系统转化成一维系统。\n",
    "\n",
    "在实际课题中，为了全面分析问题，往往提出很多与此有关的变量（或因素），因为每个变量都在不同程度上反映这个课题的某些信息。信息的大小通常用离差平方和或方差来衡量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "\n",
    "在用统计分析方法研究多变量的课题时，变量个数太多就会增加课题的复杂性。人们自然希望变量个数较少而得到的信息较多。\n",
    "\n",
    "在很多情形，变量之间是有一定的相关关系的，当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。\n",
    "\n",
    "**主成分分析是对于原先提出的所有变量，将重复的变量（关系紧密的变量）删去多余，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。**\n",
    "\n",
    "设法将原来变量重新组合成一组新的互相无关的几个综合变量，同时根据实际需要从中可以取出几个较少的综合变量尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，也是数学上用来降维的一种方法。\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41953385-f9f4c47c-7a07-11e8-9d69-761ec113cb90.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本思想\n",
    "\n",
    "**主成分分析是设法将原来众多具有一定相关性（比如P个指标）的多个指标，重新组合成一组新的互相无关的综合指标来代替原来的指标。**\n",
    "\n",
    "主成分分析，是考察多个变量间相关性一种多元统计方法，研究如何通过少数几个主成分来揭示多个变量间的内部结构，即从原始变量中导出少数几个主成分，使它们尽可能多地保留原始变量的信息，且彼此间相互独立(互不相关)。\n",
    "\n",
    "通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。\n",
    "\n",
    "### 步骤\n",
    "\n",
    "进行主成分分析主要步骤如下：\n",
    "\n",
    "1. 指标数据标准化（SPSS软件自动执行）；\n",
    "2. 指标之间的相关性判定；\n",
    "3. 确定主成分个数m；\n",
    "4. 主成分Fi表达式；\n",
    "5. 主成分Fi命名。 \n",
    "\n",
    "### 数学上详解\n",
    "\n",
    "最经典的做法就是用F1（选取的第一个线性组合，即第一个综合指标）的**方差**来表达，即$Var(F1)$越大，表示F1包含的信息越多。因此在所有的线性组合中选取的F1应该是方差最大的，故称F1为第一主成分。\n",
    "\n",
    "如果第一主成分不足以代表原来P个指标的信息，再考虑选取F2即选第二个线性组合，为了有效地反映原来信息，F1已有的信息就不需要再出现在F2中，用数学语言表达就是要求$Cov(F1, F2)=0$，则称F2为第二主成分，依此类推可以构造出第三、第四，……，第P个主成分。\n",
    "\n",
    "$F_p = a_{1i}*Z_{X1} + a_{2i}*Z_{X2} + …… + a_{pi}*Z_{Xp}$\n",
    "\n",
    "$其中 a_{1i}, a_{2i}, ……, a_{pi}(i=1,……,m)为X$的**协方差阵Σ的特征值所对应的特征向量**。\n",
    "\n",
    "$Z_{X1}, Z_{X2}, ……, Z_{Xp}是原始变量经过标准化处理的值，因为在实际应用中，往往存在指标的量纲不同，所以在计算之前须先消除量纲的影响，而将原始数据标准化。$\n",
    "\n",
    "$本文所采用的数据就存在量纲影响[注：本文指的数据标准化是指Z标准化]。$\n",
    "\n",
    "$A = (a_{ij})p×m = (a_1,a_2,…a_m)，R_{ai} = λ_ia_i，R为相关系数矩阵$，$λi、ai$是相应的** 特征值和单位特征向量**，$λ1 ≥ λ2 ≥ …≥ λp ≥ 0 。$\n",
    "\n",
    "### 要点\n",
    "\n",
    "1. 通过析取主成分显出最大的个别差异，也用来削减回归分析和聚类分析中变量的数目。\n",
    "2. 可以使用样本协方差矩阵或相关系数矩阵作为出发点进行分析。\n",
    "3. 成分的保留：Kaiser主张（1960）将特征值小于1的成分放弃，只保留特征值大于1的成分。如果能用不超过3-5个成分就能解释方差变异的80%，就算是成功。\n",
    "4. 通过对原始变量进行线性组合，得到优化的指标：把原先多个指标的计算降维为少量几个经过优化指标的计算（占去绝大部分份额）\n",
    "5. 设法将原先众多具有一定相关性的指标，重新组合为一组新的互相独立的综\n",
    "合指标，并代替原先的指标\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41953542-e7088af0-7a08-11e8-9e44-4c40834b77f9.png)\n",
    "\n",
    "详解: https://www.cnblogs.com/Ponys/p/3428270.html\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954406-b4585df6-7a0d-11e8-8a1c-a0d29d8dbbe7.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954293-2b40ab04-7a0d-11e8-9d29-c745013b014c.png)\n",
    "\n",
    "### 旋转前\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954343-723729ac-7a0d-11e8-9ace-09fbbaeba71d.png)\n",
    "\n",
    "### 旋转后\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954386-a68dec90-7a0d-11e8-988c-fe660e5ae760.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954354-7f9b472c-7a0d-11e8-9ce3-0b2af707f712.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954425-c9c90f46-7a0d-11e8-803c-a2712f2ae132.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954437-d978a898-7a0d-11e8-92a2-4b8739cfbe7e.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954443-e5b38c36-7a0d-11e8-8bd4-08a3ad914029.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 主成分分析的主要作用\n",
    "\n",
    "概括起来说，主成分分析主要由以下几个方面的作用。\n",
    "\n",
    "1. 主成分分析能降低所研究的数据空间的维数。即用研究m维的Y空间代替p维的X空间(m<p)，而低维的Y空间代替高维的x空间所损失的信息很少。即：使只有一个主成分Yl(即 m=1)时，这个Yl仍是使用全部X变量(p个)得到的。例如要计算Yl的均值也得使用全部x的均值。在所选的前m个主成分中，如果某个Xi的系数全部近似于零的话，就可以把这个Xi删除，这也是一种删除多余变量的方法。\n",
    "\n",
    "2. 有时可通过因子负荷aij的结论，弄清X变量间的某些关系。\n",
    "\n",
    "3. 多维数据的一种图形表示方法。我们知道当维数大于3时便不能画出几何图形，多元统计研究的问题大都多于3个变量。要把研究的问题用图形表示出来是不可能的。然而，经过主成分分析后，我们可以选取前两个主成分或其中某两个主成分，根据主成分的得分，画出n个样品在二维平面上的分布况，由图形可直观地看出各样品在主分量中的地位，进而还可以对样本进行分类处理，可以由图形发现远离大多数样本点的离群点。\n",
    "\n",
    "4. 由主成分分析法构造回归模型。即把各主成分作为新自变量代替原来自变量x做回归分析。\n",
    "\n",
    "5. 用主成分分析筛选回归变量。回归变量的选择有着重的实际意义，为了使模型本身易于做结构分析、控制和预报，好从原始变量所构成的子集合中选择最佳变量，构成最佳变量集合。用主成分分析筛选变量，可以用较少的计算量来选择量，获得选择最佳变量子集合的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优缺点\n",
    "\n",
    "优点：降低数据的复杂性，识别最重要的多个特征\n",
    "\n",
    "缺点：不一定需要，且有可能损失有用信息\n",
    "\n",
    "适用数据类型：狭长型的数据（数值型数据），球型数据则不适合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算过程\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41953740-dc46f4f2-7a09-11e8-8fbd-90a6add49e7b.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python中的PCA实现\n",
    "```\n",
    "import sklearn as sk\n",
    "\n",
    "sk.PCA(n_components=None, # 需要保留的主成分数目; \"mle\" 自动选择最佳的主成分数据; 0~1:例如0.8,表示选择解释超过80%的主成分数量\n",
    "       copy=True, # 不覆盖原数据\n",
    "       whiten=False) # FALSE不白化，保留每个特征的方差（保留精度）;True白化，使得每个特征具有相同的方差。\n",
    "```\n",
    "\n",
    "属性\n",
    "\n",
    " - components\n",
    " - explained_variance_ratio\n",
    " - mean_\n",
    " - n_components\n",
    " - noise_variance\n",
    "\n",
    "方法\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/26344632/41954010-78104fae-7a0b-11e8-88b2-27aa50b4fa26.png)\n",
    "\n",
    "参考链接:\n",
    "\n",
    "参数详解:http://www.cnblogs.com/eczhou/p/5433856.html\n",
    "\n",
    "白化： http://deeplearning.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
